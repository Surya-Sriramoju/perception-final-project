{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout_rate):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=dropout_rate)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(self.embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_out = self.attention(x, x, x)[0]\n",
    "        x = x + self.dropout(attention_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        feed_forward_out = self.feed_forward(x)\n",
    "        x = x + self.dropout(feed_forward_out)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim, num_heads, hidden_dim, num_layers, num_classes, dropout_rate):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size=img_size, patch_size=patch_size, in_channels=in_channels,\n",
    "                                              embed_dim=embed_dim)\n",
    "\n",
    "        self.transformer_encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim=embed_dim, num_heads=num_heads, hidden_dim=hidden_dim,\n",
    "                               dropout_rate=dropout_rate) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.segmentation_head = nn.Conv2d(embed_dim, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        class_token = self.class_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        for encoder in self.transformer_encoder:\n",
    "            x = encoder(x)\n",
    "\n",
    "        x = self.dropout(x[:, 0])\n",
    "        x = x.view(x.size(0), -1, int(x.size(-1) ** 0.5), int(x.size(-1) ** 0.5))\n",
    "        x = self.segmentation_head(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mys/ENPM_673/FInal_Project/Dataset/train/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Dataset and DataLoader\u001b[39;00m\n\u001b[1;32m     19\u001b[0m dataset_root \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/mys/ENPM_673/FInal_Project/Dataset\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m train_dataset \u001b[39m=\u001b[39m CityscapesDataset(root_dir\u001b[39m=\u001b[39;49mdataset_root, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, transform\u001b[39m=\u001b[39;49mToTensor())\n\u001b[1;32m     21\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Model\u001b[39;00m\n",
      "File \u001b[0;32m~/ENPM_673/FInal_Project/Data.py:13\u001b[0m, in \u001b[0;36mCityscapesDataset.__init__\u001b[0;34m(self, root_dir, split, transform)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit, \u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_filenames \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimages_dir)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mys/ENPM_673/FInal_Project/Dataset/train/images'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from Data import CityscapesDataset\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "num_classes = 20\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset_root = \"/home/mys/ENPM_673/FInal_Project/Dataset\"\n",
    "train_dataset = CityscapesDataset(root_dir=dataset_root, split=\"train\", transform=ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = VisionTransformer(\n",
    "    img_size=512,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    embed_dim=768,\n",
    "    num_heads=12,\n",
    "    hidden_dim=3072,\n",
    "    num_layers=12,\n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=0.1\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"vision_transformer_segmentation.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
